{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERGENCE_THRESHOLD = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid:\n",
    "    def __init__(self, n_rows=3, n_cols=3, start_idx=(0,0)):\n",
    "        self.rows = n_rows\n",
    "        self.cols = n_cols\n",
    "        # i: row idx\n",
    "        self._i = start_idx[0]\n",
    "        # j: col idx\n",
    "        self._j = start_idx[1]\n",
    "        self._all_states = {}\n",
    "    \n",
    "    def print_info(self):\n",
    "        print(\"Grid world information:\")\n",
    "        print(\"  # of rows: {}\".format(self.rows))\n",
    "        print(\"  # of cols: {}\".format(self.cols))\n",
    "        print(\"  Current index (i,j) = ({},{})\".format(self._i, self._j))\n",
    "        print(\"  Actions:\")\n",
    "        for a in self._actions:\n",
    "            print(\"      {}: {}\".format(a, self._actions[a]))            \n",
    "        print(\"  Rewards:\")\n",
    "        for r in self._rewards:\n",
    "            print(\"      {}: {}\".format(r, self._rewards[r]))\n",
    "        \n",
    "    def set_rewards(self, rewards):\n",
    "        \"\"\"\n",
    "        Set rewards. Rewards is a dictionary {key: val}\n",
    "            - Key: state (i,j) tuples\n",
    "            - Value: reward_value (a scalar)\n",
    "        \"\"\"\n",
    "        self._rewards = rewards\n",
    "    \n",
    "    def get_reward(self, st):\n",
    "        \"\"\"\n",
    "        Return the reward value for a given state. \n",
    "        st: a tuple (i,j). \n",
    "        if st not found in rewards dictionary, return 0.\n",
    "        \"\"\"\n",
    "        if st in self._rewards:\n",
    "            return self._rewards[st]\n",
    "        # print(\"Warning. Could not find state {} in rewards dictionary. Returning 0 reward.\")\n",
    "        # set default reward as zero\n",
    "        return 0\n",
    "    \n",
    "    def set_actions(self, actions):\n",
    "        \"\"\"\n",
    "        Set actions. Actions is a dictionary {key: val}\n",
    "            - Key: state (i,j) tuples\n",
    "            - Value: A (a list of possible actions from current cell at (i,j))\n",
    "        \"\"\"\n",
    "        self._actions = actions\n",
    "    \n",
    "    def get_actions(self, state):\n",
    "        if state in self._actions:\n",
    "            return self._actions[state]\n",
    "        return None\n",
    "        \n",
    "    def set_state(self, st):\n",
    "        \"\"\"\n",
    "        Set a new state. For this environment, it's the the index of the new cell at (i,j).\n",
    "        st is a tuple\n",
    "        \"\"\"\n",
    "        self._i = st[0]\n",
    "        self._j = st[1]\n",
    "    \n",
    "    def get_current_state(self):\n",
    "        \"\"\"\n",
    "        Return current state as an index tuple: (i,j).\n",
    "        \"\"\"\n",
    "        return (self._i, self._j)\n",
    "    \n",
    "    def get_all_states(self):\n",
    "        return set(self._actions.keys()) | set(self._rewards.keys())\n",
    "    \n",
    "    def _update_state_using_action(self, action, mode='default'):\n",
    "        \"\"\"\n",
    "        Updates the index (i,j) given action. \n",
    "        Accepted values for 'action':\n",
    "            'U': Up \n",
    "            'D': Down\n",
    "            'L': Left\n",
    "            'R': Right\n",
    "        Accepted values for mode:\n",
    "            'default': moves forward via action.\n",
    "            'undo': moves the piece backwards (to reset the grid back to the original place)\n",
    "        \"\"\"\n",
    "        # mapping dictionay between actions and increments from (i,j) to new cell.\n",
    "        action_increment = {\n",
    "            'U': (+1,0),\n",
    "            'D': (-1,0),\n",
    "            'R': (0,+1),\n",
    "            'L': (0,-1)\n",
    "        }\n",
    "        action_inc = action_increment[action.upper()]\n",
    "        # Default mode: Move the piece forward\n",
    "        if mode.lower() == 'default': \n",
    "            self._i += action_inc[0]\n",
    "            self._j += action_inc[1]\n",
    "        # Undo move mode. Move the piece backwards to the original place.\n",
    "        elif mode.lower() == 'undo':\n",
    "            self._i -= action_inc[0]\n",
    "            self._j -= action_inc[1]\n",
    "        else:\n",
    "            raise ValueError('Unexpected value for \"mode\": {}. Use \"default\" or \"undo\".'.format(mode))\n",
    "    \n",
    "    def move(self, action):\n",
    "        \"\"\"\n",
    "        Update the state (i,j) based on the given action. \n",
    "        Return: \n",
    "            reward value for the updated state (after taking the action.)\n",
    "        \"\"\"\n",
    "        # return (i,j) tuple as current state\n",
    "        current_state = self.get_current_state()\n",
    "        if action not in self._actions[current_state]:\n",
    "            raise ValueError('Cannot move to new cell since action {} is not in possible actions.'.format(action))\n",
    "        else:\n",
    "            # move to the next cell based on current action. \n",
    "            self._update_state_using_action(action, mode='default')\n",
    "            \n",
    "        # return the reward value for the new state\n",
    "        return self.get_reward((self._i,self._j))\n",
    "    \n",
    "    def undo_move(self, action):\n",
    "        \"\"\"\n",
    "        Undo move based on the action. \n",
    "        \"\"\"\n",
    "        self._update_state_using_action(action, mode='undo')\n",
    "        # if current state no in all states, then there is an error. \n",
    "        assert(self.get_current_state() in self.get_all_states())\n",
    "\n",
    "    def is_terminal(self, st):\n",
    "        \"\"\"\n",
    "        If current state not in action keys, it's a terminal state.\n",
    "        TODO: There should be a better way to define this.\n",
    "        \"\"\"\n",
    "        if st not in self._actions.keys():\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def game_over(self):\n",
    "        \"\"\"\n",
    "        If current state not in action keys, game over. \n",
    "        We have reached a terminal state.\n",
    "        \"\"\"\n",
    "        if self.get_current_state() not in self._actions.keys():\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_standard_grid(n_rows=3, n_cols=4, start_idx=(0,0),\\\n",
    "                         banned_cells=[(1,1)],\\\n",
    "                         terminal_cells_and_rewards={(2,3):1, (1,3):-1}):\n",
    "    \n",
    "    def is_setup_correct():\n",
    "        \"\"\"\n",
    "        Do sanity checks on the given grid setup. \n",
    "        \"\"\"\n",
    "        if not terminal_cells_and_rewards:\n",
    "            print('Error. reward_cells dictionary cannot be empty. Set at least two cells with positive and negative rewards.')\n",
    "            return False\n",
    "\n",
    "        if any(_out_of_bounds(cell) for cell in terminal_cells_and_rewards.keys()):\n",
    "            print('Error. Reward cell dictionary includes out-of-bounds cell.')\n",
    "            return False\n",
    "\n",
    "        if banned_cells and any(_out_of_bounds(cell) for cell in banned_cells):\n",
    "            print('Error. Banned cell includes out-of-bounds cell.')\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def _out_of_bounds(cell_idx):\n",
    "        i = cell_idx[0]\n",
    "        j = cell_idx[1]\n",
    "        if i >= n_rows or i < 0 or j >= n_cols or j < 0:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _is_banned_cell(cell_idx):\n",
    "        if banned_cells and cell_idx in banned_cells:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _is_reward_cell(cell_idx):\n",
    "        if cell_idx in terminal_cells_and_rewards.keys():\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _generate_possible_actions():\n",
    "        \"\"\"\n",
    "        Go through entire grid. Create a dictionay of \n",
    "        cells (as keys) and a list of feasible actions. \n",
    "        Action dictionary should NOT include terminal cells/states.\n",
    "        Also, it will not include the 'banned cells'. These are 'dead cells' which user is not allowed to enter.\n",
    "        Returns: \n",
    "        A dictionary in the form of:\n",
    "            {(i,j): ['U', 'D', ...]}\n",
    "        \"\"\"\n",
    "        action_increment = {\n",
    "            'U': (+1,0),\n",
    "            'D': (-1,0),\n",
    "            'R': (0,+1),\n",
    "            'L': (0,-1)\n",
    "        }\n",
    "        actions = {}\n",
    "        for i in range(n_rows):\n",
    "            for j in range(n_cols):\n",
    "                current_cell = (i,j)\n",
    "                # If current cell is a reward_cell (terminal state) or a banned cell (not allowed cell),\n",
    "                # don't add it to action dictionay.\n",
    "                if _is_reward_cell(current_cell) or _is_banned_cell(current_cell):\n",
    "                    continue\n",
    "                feasible_action_list = []\n",
    "                for act in action_increment:\n",
    "                    increment = action_increment[act]\n",
    "                    next_cell = (i+increment[0], j+increment[1])\n",
    "                    if not _is_banned_cell(next_cell) and not _out_of_bounds(next_cell):\n",
    "                        feasible_action_list.append(act)\n",
    "                actions[current_cell] = feasible_action_list\n",
    "        return actions\n",
    "    \n",
    "    if not is_setup_correct():\n",
    "        print('Error setting up the grid.')\n",
    "        return None\n",
    "\n",
    "    # create the grid (environment)\n",
    "    g = Grid(n_rows=n_rows, n_cols=n_cols, start_idx=start_idx)\n",
    "    rewards = terminal_cells_and_rewards\n",
    "    g.set_rewards(rewards)\n",
    "\n",
    "    actions = _generate_possible_actions()\n",
    "    g.set_actions(actions)\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = create_standard_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid world information:\n",
      "  # of rows: 3\n",
      "  # of cols: 4\n",
      "  Current index (i,j) = (0,0)\n",
      "  Actions:\n",
      "      (0, 0): ['U', 'R']\n",
      "      (0, 1): ['R', 'L']\n",
      "      (0, 2): ['U', 'R', 'L']\n",
      "      (0, 3): ['U', 'L']\n",
      "      (1, 0): ['U', 'D']\n",
      "      (1, 2): ['U', 'D', 'R']\n",
      "      (2, 0): ['D', 'R']\n",
      "      (2, 1): ['R', 'L']\n",
      "      (2, 2): ['D', 'R', 'L']\n",
      "  Rewards:\n",
      "      (2, 3): 1\n",
      "      (1, 3): -1\n"
     ]
    }
   ],
   "source": [
    "g.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_values(values, grid):\n",
    "    for i in reversed(range(grid.rows)):\n",
    "        print('-'*(6*grid.cols+1))\n",
    "        print('|', end=\"\")\n",
    "        for j in range(grid.cols):\n",
    "            v = values.get((i,j), 0)\n",
    "            if v > 0:\n",
    "                print(\"+%.2f|\" % v, end=\"\")\n",
    "            elif v < 0:\n",
    "                print(\"%.2f|\" % v, end=\"\") \n",
    "            else:\n",
    "                print(\" 0.00|\", end=\"\")\n",
    "        print(\"\")\n",
    "    print('-'*(6*grid.cols+1))\n",
    "\n",
    "    \n",
    "def print_policy(policies, grid):\n",
    "    for i in reversed(range(grid.rows)):\n",
    "        print('-'*(4*grid.cols+1))\n",
    "        print('|', end=\"\")\n",
    "        for j in range(grid.cols):\n",
    "            action = policies.get((i,j), '.')\n",
    "            print(\" %s |\" % action, end=\"\") \n",
    "        print(\"\")\n",
    "    print('-'*(4*grid.cols+1))   \n",
    "\n",
    "    \n",
    "def generate_fixed_policy():\n",
    "    policy = {\n",
    "        (2, 0): 'U',\n",
    "        (1, 0): 'U',\n",
    "        (0, 0): 'R',\n",
    "        (0, 1): 'R',\n",
    "        (0, 2): 'R',\n",
    "        (1, 2): 'R',\n",
    "        (2, 1): 'R',\n",
    "        (2, 2): 'R',\n",
    "        (2, 3): 'U',\n",
    "    }\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = create_standard_grid()\n",
    "states = grid.get_all_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 with deltaV 0.011272908460062847\n",
      "Iteration 20 with deltaV 0.002489071127286857\n",
      "Policy iteration converged after 27 iterations.\n"
     ]
    }
   ],
   "source": [
    "# Uniform randomly chosen actions\n",
    "# discount factor\n",
    "gamma = 1.0\n",
    "\n",
    "# initialize Value functions\n",
    "V = {}\n",
    "for st in states:\n",
    "    V[st] = 0\n",
    "\n",
    "count = 0\n",
    "while True:\n",
    "    biggest_delta_V = 0.0\n",
    "    for st in states:\n",
    "        old_v = V[st]\n",
    "        states_with_possible_actions = grid._actions.keys()\n",
    "        # go through all states that have possible actions. \n",
    "        # Essentially, exclude terminal states and dead cells.\n",
    "        if st in states_with_possible_actions:\n",
    "            new_v = 0\n",
    "            actions = grid.get_actions(st)\n",
    "            # each action has equal probability of being chosen\n",
    "            prob_action = 1.0/len(actions)\n",
    "            for act in actions:\n",
    "                # set the state back to st\n",
    "                grid.set_state(st)\n",
    "                reward = grid.move(act)\n",
    "                # state is now changed since we made a move based on a possible action\n",
    "                st_next = grid.get_current_state()\n",
    "                # update new_v for st\n",
    "                new_v += prob_action * (reward + gamma * V[st_next])\n",
    "            V[st] = new_v\n",
    "            biggest_delta_V = max(biggest_delta_V, np.abs(old_v - new_v))\n",
    "    count += 1\n",
    "    if biggest_delta_V < CONVERGENCE_THRESHOLD:\n",
    "        print(\"Policy iteration converged after {} iterations.\".format(count))\n",
    "        break\n",
    "    if count % 10 == 0:\n",
    "        print(\"Iteration {} with deltaV {}\".format(count, biggest_delta_V))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "|-0.03|+0.09|+0.22| 0.00|\n",
      "-------------------------\n",
      "|-0.16| 0.00|-0.44| 0.00|\n",
      "-------------------------\n",
      "|-0.29|-0.41|-0.54|-0.77|\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_values(V, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
