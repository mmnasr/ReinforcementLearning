{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from Grid import create_standard_grid\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERGENCE_THRESHOLD = 1e-3\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'R', 'L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_values(values, grid, title=None):\n",
    "    if title:\n",
    "        print(title)\n",
    "        \n",
    "    for i in reversed(range(grid.rows)):\n",
    "        print('-'*(6*grid.cols+1))\n",
    "        print('|', end=\"\")\n",
    "        for j in range(grid.cols):\n",
    "            v = values.get((i,j), 0)\n",
    "            if v > 0:\n",
    "                print(\"+%.2f|\" % v, end=\"\")\n",
    "            elif v < 0:\n",
    "                print(\"%.2f|\" % v, end=\"\") \n",
    "            else:\n",
    "                print(\" 0.00|\", end=\"\")\n",
    "        print(\"\")\n",
    "    print('-'*(6*grid.cols+1))\n",
    "\n",
    "    \n",
    "def print_policy(policies, grid, title=None):\n",
    "    if title:\n",
    "        print(title)\n",
    "        \n",
    "    for i in reversed(range(grid.rows)):\n",
    "        print('-'*(4*grid.cols+1))\n",
    "        print('|', end=\"\")\n",
    "        for j in range(grid.cols):\n",
    "            action = policies.get((i,j), '.')\n",
    "            print(\" %s |\" % action, end=\"\") \n",
    "        print(\"\")\n",
    "    print('-'*(4*grid.cols+1))   \n",
    "\n",
    "    \n",
    "def generate_fixed_policy():\n",
    "    \"\"\"\n",
    "    Return a fixed policy to win the game (using only one action for each state).\n",
    "    This doesn't have to be unique.\n",
    "    \"\"\"\n",
    "    policy = {\n",
    "        (0, 0): 'R',\n",
    "        (1, 0): 'U',\n",
    "        (2, 0): 'R',\n",
    "        (0, 1): 'R',\n",
    "        (2, 1): 'R',\n",
    "        (0, 2): 'U',\n",
    "        (1, 2): 'U',\n",
    "        (2, 2): 'R',\n",
    "        (0, 3): 'L'\n",
    "    }\n",
    "    return policy\n",
    "\n",
    "def generate_random_policy(grid):\n",
    "    \"\"\"\n",
    "    Generate a random policy using available actions for each cell.\n",
    "    \"\"\"\n",
    "    states = grid._actions.keys()\n",
    "    policy_ = {}\n",
    "    for st in states:\n",
    "        policy_[st] = np.random.choice(grid._actions[st])\n",
    "        \n",
    "    return policy_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_policy_iteration_uniform_actions(grid, gamma_=1.0):\n",
    "    \"\"\"\n",
    "    Run policy iterations assuming a uniform probability \n",
    "    of choosing possible actions at each cell.\n",
    "    Return converged value function V (a dictionary of states (cell) and gain)\n",
    "        V: {(i,j): value}\n",
    "    \"\"\"\n",
    "    gamma = gamma_\n",
    "    states = grid.get_all_states()\n",
    "    # initialize Value functions\n",
    "    V = {}\n",
    "    for st in states:\n",
    "        V[st] = 0\n",
    "    \n",
    "    count = 0\n",
    "    # iterate until maximum deltaV is below a threshold.\n",
    "    while True:\n",
    "        biggest_delta_V = 0.0\n",
    "        for st in states:\n",
    "            old_v = V[st]\n",
    "            states_with_possible_actions = grid._actions.keys()\n",
    "            # go through all states that have possible actions. \n",
    "            # Essentially, exclude terminal states and dead cells.\n",
    "            if st in states_with_possible_actions:\n",
    "                new_v = 0\n",
    "                actions = grid.get_actions(st)\n",
    "                # each action has equal probability of being chosen\n",
    "                prob_action = 1.0/len(actions)\n",
    "                # go through all actions and update V[st]\n",
    "                for act in actions:\n",
    "                    # set the state back to st\n",
    "                    grid.set_state(st)\n",
    "                    reward = grid.move(act)\n",
    "                    # state is now changed since we made a move based on a possible action\n",
    "                    st_next = grid.get_current_state()\n",
    "                    # update new_v for st\n",
    "                    new_v += prob_action * (reward + gamma * V[st_next])\n",
    "                V[st] = new_v\n",
    "                # maximum different in successive iterations\n",
    "                biggest_delta_V = max(biggest_delta_V, np.abs(old_v - new_v))\n",
    "        count += 1\n",
    "        if biggest_delta_V < CONVERGENCE_THRESHOLD:\n",
    "            print(\"Policy iteration converged after {} iterations.\".format(count))\n",
    "            break\n",
    "            \n",
    "        if count % 5 == 0:\n",
    "            print(\"    Iteration {} with deltaV {}\".format(count, biggest_delta_V))\n",
    "            \n",
    "    return V\n",
    "\n",
    "\n",
    "def run_fixed_policy(grid, fixed_policy=None, gamma_=0.9):\n",
    "    \"\"\"\n",
    "    We assume a fixed policy is given (for actions at each state/cell).\n",
    "    Iterate to find the value function for each state.\n",
    "    Return converged value function V (a dictionary of states (cell), gain)\n",
    "        V: {(i,j): value}\n",
    "    \"\"\"\n",
    "    gamma = gamma_\n",
    "    states = grid.get_all_states()\n",
    "    # initialize Value functions\n",
    "    V = {}\n",
    "    for st in states:\n",
    "        V[st] = 0\n",
    "\n",
    "    count = 0\n",
    "    while True:\n",
    "        biggest_delta_V = 0.0\n",
    "        for st in states:\n",
    "            old_v = V[st]\n",
    "            states_from_fixed_policy = fixed_policy.keys()\n",
    "            # Go through all states that have possible actions. \n",
    "            # Essentially, exclude terminal states and dead cells.\n",
    "            if st in states_from_fixed_policy:\n",
    "                # set state tp current statu\n",
    "                grid.set_state(st)\n",
    "                # just one action per state (fixed policy)\n",
    "                act = fixed_policy[st]\n",
    "                reward = grid.move(act)\n",
    "                # state is now changed since we made a move given action act.\n",
    "                st_next = grid.get_current_state()\n",
    "                # update V[st]\n",
    "                new_v = reward + gamma * V[st_next]\n",
    "                V[st] = new_v\n",
    "                # find the biggest change (for convergence checking)\n",
    "                biggest_delta_V = max(biggest_delta_V, np.abs(old_v - new_v))\n",
    "        count += 1\n",
    "        if biggest_delta_V < CONVERGENCE_THRESHOLD:\n",
    "            print(\"Policy iteration converged after {} iterations.\".format(count))\n",
    "            break\n",
    "        if count % 10 == 0:\n",
    "            print(\"Iteration {} with deltaV {}\".format(count, biggest_delta_V))\n",
    "            \n",
    "    return V\n",
    "\n",
    "def run_random_policy_iteration_and_improvement(grid, policy_, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Iterate both on policy and value functions in two sub-steps:\n",
    "    1- policy evaluation: Given current policy, find the value function for all states until V converges. \n",
    "    2- policy improvement: Given current policy/actions, find the best action resulting in best value function. \n",
    "    Convergence: When actions in two successive policy iterations don't change. \n",
    "    \"\"\"\n",
    "    def _initialize_V():\n",
    "        \"\"\"\n",
    "        Initialize V[st]\n",
    "        \"\"\"\n",
    "        V = {}\n",
    "        states = grid.get_all_states()\n",
    "        for st in states:\n",
    "            if st in grid._actions.keys():\n",
    "                V[st] = np.random.random()\n",
    "            else: # terminal state\n",
    "                V[st] = 0\n",
    "                \n",
    "        return V\n",
    "    \n",
    "    def _do_policy_evaluation():\n",
    "        \"\"\"\n",
    "        Run one step of policy evaluation.\n",
    "        Go through all states, pick action, move to next state, and update V using Bellman equation.\n",
    "        Return the maximum deltaV (V_{k+1},V_k) amongst all states.\n",
    "        \"\"\"\n",
    "        # get all states\n",
    "        states = grid.get_all_states()\n",
    "        delta_V_max = 0.0\n",
    "        for st in states:\n",
    "            old_v = V[st]\n",
    "            policy_states = policy.keys()\n",
    "            if st in policy_states:\n",
    "                grid.set_state(st)\n",
    "                act = policy[st]\n",
    "                reward = grid.move(act)\n",
    "                new_st = grid.get_current_state()\n",
    "                new_v = reward + gamma * V[new_st]\n",
    "                V[st] = new_v\n",
    "                delta_V_max = max(delta_V_max, np.abs(old_v - new_v))\n",
    "                \n",
    "        return delta_V_max\n",
    "    \n",
    "    def _do_policy_improvement():\n",
    "        \"\"\"\n",
    "        Go through all possible actions in current policy, find the 'best action'\n",
    "        for state which V (value function) is maximum (gain).\n",
    "        V is updated using Bellman equation. Upon convergence, return V,policy\n",
    "        \"\"\"\n",
    "        def __find_best_action(st):\n",
    "            \"\"\"\n",
    "            Find best action which maximizes value function V (via Bellman equation)\n",
    "            \"\"\"\n",
    "            old_act = policy[st]\n",
    "            best_act = None\n",
    "            best_value = float('-inf')\n",
    "            for act in grid.get_actions(st):\n",
    "                grid.set_state(st)\n",
    "                reward = grid.move(act)\n",
    "                new_st = grid.get_current_state()\n",
    "                v = reward + gamma * V[new_st]\n",
    "                # find the action which yields in maximum value function\n",
    "                if v > best_value:\n",
    "                    best_value = v\n",
    "                    best_act = act\n",
    "                    \n",
    "            return best_act\n",
    "        \n",
    "        states = grid.get_all_states()\n",
    "        converged = True\n",
    "        for st in states:\n",
    "            # if state\n",
    "            policy_states = policy.keys()\n",
    "            if st in policy_states:\n",
    "                old_act = policy[st]\n",
    "                new_act = __find_best_action(st)\n",
    "                policy[st] = new_act\n",
    "                if old_act != new_act:\n",
    "                    converged = False\n",
    "                    return policy, converged\n",
    "                \n",
    "        # if we have reached here, policy is converged. \n",
    "        return policy, converged\n",
    "\n",
    "    V = _initialize_V()\n",
    "    policy = copy.deepcopy(policy_)\n",
    "    count_1 = 0\n",
    "    while True:\n",
    "        count = 0\n",
    "        # First, iterate on policy evaluation until convergence reached.\n",
    "        while True:\n",
    "            delta_V = _do_policy_evaluation()\n",
    "            count += 1\n",
    "            if delta_V < CONVERGENCE_THRESHOLD:\n",
    "                print(\"   Policy evaluation converged in {} iterations with final error {}.\".format(count, delta_V))\n",
    "                break\n",
    "            elif count % 5 == 0:\n",
    "                pass\n",
    "                # print(\"  Policy evaluation iteration: {}.\".format(count))\n",
    "\n",
    "        policy, is_policy_converged = _do_policy_improvement()\n",
    "        count_1 += 1\n",
    "        if is_policy_converged:\n",
    "            print(\"Policy improvement converged in {} iterations.\".format(count_1))\n",
    "            break\n",
    "        elif count_1 % 5 == 0:\n",
    "            pass\n",
    "            # print(\"  Policy improvement iteration: {}.\".format(count_1))\n",
    "\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_uniform():\n",
    "    print(\"Experiment 1: Uniform probablity value function iteration.\")\n",
    "    grid = create_standard_grid()\n",
    "    V_uniform = run_policy_iteration_uniform_actions(grid, gamma_=1)\n",
    "    print_values(V_uniform, grid, title=\"Value functions- Uniform probability.\")\n",
    "    print(\"*\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_fixed_policy():\n",
    "    print(\"Experiment 2: Fixed policy. Value function iteration\")\n",
    "    grid = create_standard_grid()\n",
    "    fixed_policy = generate_fixed_policy()\n",
    "    print_policy(fixed_policy, grid, title=\"Fixed (given) policy\")\n",
    "    V_fixed_policy = run_fixed_policy(grid, fixed_policy=fixed_policy, gamma_=0.9)\n",
    "    print_values(V_fixed_policy, grid, title=\"Value function- Fixed policy\")    \n",
    "    print(\"*\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_policy_evaluation_improvement():\n",
    "    print(\"Experiment 3: Random policy w/ policy evaluation/improvement.\")\n",
    "    grid = create_standard_grid(step_negative_reward=-0.1)\n",
    "    policy0 = generate_random_policy(grid)\n",
    "    print_policy(policy0, grid, title=\"Random initial policy\")\n",
    "    policy_conv, V_conv = run_random_policy_iteration_and_improvement(grid, policy_=policy0, gamma=0.9)\n",
    "    print_values(V_conv, grid, title=\"Converged value functions.\")\n",
    "    print_policy(policy_conv, grid, title=\"Converged policy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1: Uniform probablity value function iteration.\n",
      "    Iteration 5 with deltaV 0.04093673792104857\n",
      "    Iteration 10 with deltaV 0.011272908460062847\n",
      "    Iteration 15 with deltaV 0.005267978380583682\n",
      "    Iteration 20 with deltaV 0.002489071127286857\n",
      "    Iteration 25 with deltaV 0.001176767267569323\n",
      "Policy iteration converged after 27 iterations.\n",
      "Value functions- Uniform probability.\n",
      "-------------------------\n",
      "|-0.03|+0.09|+0.22| 0.00|\n",
      "-------------------------\n",
      "|-0.16| 0.00|-0.44| 0.00|\n",
      "-------------------------\n",
      "|-0.29|-0.41|-0.54|-0.77|\n",
      "-------------------------\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "run_experiment_uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 2: Fixed policy. Value function iteration\n",
      "Fixed (given) policy\n",
      "-----------------\n",
      "| R | R | R | . |\n",
      "-----------------\n",
      "| U | . | U | . |\n",
      "-----------------\n",
      "| R | R | U | L |\n",
      "-----------------\n",
      "Policy iteration converged after 4 iterations.\n",
      "Value function- Fixed policy\n",
      "-------------------------\n",
      "|+0.81|+0.90|+1.00| 0.00|\n",
      "-------------------------\n",
      "|+0.73| 0.00|+0.90| 0.00|\n",
      "-------------------------\n",
      "|+0.66|+0.73|+0.81|+0.73|\n",
      "-------------------------\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "run_experiment_fixed_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 3: Random policy w/ policy evaluation/improvement.\n",
      "Random initial policy\n",
      "-----------------\n",
      "| R | L | R | . |\n",
      "-----------------\n",
      "| U | . | U | . |\n",
      "-----------------\n",
      "| U | L | L | U |\n",
      "-----------------\n",
      "   Policy evaluation converged in 28 iterations with final error 0.0008669443650253239.\n",
      "   Policy evaluation converged in 1 iterations with final error 0.0007022249356705146.\n",
      "   Policy evaluation converged in 1 iterations with final error 0.0005688021978931257.\n",
      "   Policy evaluation converged in 2 iterations with final error 0.0003731911220375972.\n",
      "   Policy evaluation converged in 2 iterations with final error 0.0002448506951688856.\n",
      "   Policy evaluation converged in 2 iterations with final error 0.0.\n",
      "   Policy evaluation converged in 2 iterations with final error 0.0.\n",
      "   Policy evaluation converged in 2 iterations with final error 0.0.\n",
      "   Policy evaluation converged in 2 iterations with final error 0.0.\n",
      "Policy improvement converged in 9 iterations.\n",
      "Converged value functions.\n",
      "-------------------------\n",
      "|+0.62|+0.80|+1.00| 0.00|\n",
      "-------------------------\n",
      "|+0.46| 0.00|+0.80| 0.00|\n",
      "-------------------------\n",
      "|+0.31|+0.46|+0.62|+0.46|\n",
      "-------------------------\n",
      "Converged policy.\n",
      "-----------------\n",
      "| R | R | R | . |\n",
      "-----------------\n",
      "| U | . | U | . |\n",
      "-----------------\n",
      "| U | R | U | L |\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "run_experiment_policy_evaluation_improvement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
